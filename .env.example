# Lucia Agent Host - Environment Variables Template
# 
# Instructions:
#   1. Copy this file to .env in the project root: cp .env.example .env
#   2. Edit .env with your actual configuration values
#   3. NEVER commit .env to git (it's in .gitignore)
#   4. Keep .env.example as documentation for required variables
#
# Configuration Format:
#   - Variables are case-sensitive
#   - Strings with spaces need quotes: "value with spaces"
#   - URLs must include protocol: http:// or https://
#   - Connection strings use semicolon-separated key=value format
#
# Provider Guide:
#   - openai: OpenAI API (requires API key from https://platform.openai.com/api-keys)
#   - azureopenai: Azure OpenAI (supports embeddings - required for semantic search)
#   - ollama: Local LLM via Ollama (no API key needed, runs locally)
#   - azureinference: Azure AI Inference (alternative Azure model provider)
#

################################################################################
# ENVIRONMENT & RUNTIME
################################################################################

# Application environment (Development, Staging, Production)
LUCIA_ENV=Development

# Log level (Debug, Information, Warning, Error, Critical)
LUCIA_LOG_LEVEL=Information

# Enable telemetry collection - this is for OTEL, dump it to where you want, we don't collect it!
LUCIA_ENABLE_TELEMETRY=true


################################################################################
# HOME ASSISTANT INTEGRATION
################################################################################

# Home Assistant instance URL
# Examples:
#   Local network: http://192.168.1.100:8123
#   Docker network: http://homeassistant:8123
#   External: https://home.example.com
HOMEASSISTANT_URL=http://homeassistant:8123

# Home Assistant access token
# Generate at: Settings → Developers Tools → Create Token
# ⚠️  KEEP THIS SECRET - Never share or commit to git
HOMEASSISTANT_ACCESS_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiI3YzdjN2M3YyIsImlhdCI6MTcyNTAxMTAwMCwiZXhwIjoxODAwMDAwMDAwLCJzdWIiOiJsdWNpYS1zZXJ2aWNlIn0.PLACEHOLDER_REPLACE_WITH_ACTUAL_TOKEN

# Home Assistant connection timeout (seconds)
HOMEASSISTANT_TIMEOUT=30

# Device discovery method (semantic, name, id)
HOMEASSISTANT_DISCOVERY_METHOD=semantic


################################################################################
# REDIS CONFIGURATION
################################################################################

# Redis connection string for task persistence
# Format: redis://[user:password@]host[:port][/database]
# Examples:
#   Docker network: redis://redis:6379
#   Local: redis://localhost:6379
#   With password: redis://:mypassword@redis:6379
#   Specific DB: redis://redis:6379/1
REDIS_CONNECTION_STRING=redis://redis:6379

# Redis operations timeout (milliseconds)
REDIS_TIMEOUT=5000

# Redis key prefix (for multi-tenant or shared Redis)
REDIS_KEY_PREFIX=lucia:

# Task persistence TTL (hours) - older tasks auto-deleted
REDIS_PERSISTENCE_TTL=24


################################################################################
# LLM & CHAT MODEL CONFIGURATION
################################################################################

# Unified connection string for chat model provider
# Format: Endpoint=<url>;AccessKey=<key>;Model=<model>;Provider=<provider>
#
# OPENAI CONFIGURATION:
#   - Get API key: https://platform.openai.com/api-keys
#   - Endpoint: https://api.openai.com/v1
#   - Model: gpt-4o (recommended), gpt-4-turbo, gpt-4, gpt-3.5-turbo
#   - No embeddings support (planned)
ConnectionStrings__chat-model=Endpoint=https://api.openai.com/v1;AccessKey=sk-proj-YOUR_OPENAI_API_KEY;Model=gpt-4o;Provider=openai

# OLLAMA CONFIGURATION (local LLM - no API key needed):
#   - Model: llama3.2, phi3:mini, mistral, neural-chat, etc.
#   - Run locally: ollama pull llama3.2 && ollama serve
#   - Endpoint: http://localhost:11434 (local) or http://ollama:11434 (Docker)
# UNCOMMENT TO USE OLLAMA INSTEAD:
# ConnectionStrings__chat-model=Endpoint=http://ollama:11434;AccessKey=ollama;Model=llama3.2;Provider=ollama

# AZURE OPENAI CONFIGURATION (embeddings support):
#   - Get details from Azure Portal → Cognitive Services → Keys and Endpoint
#   - Model: Deployment name (e.g., gpt-4-deployment)
#   - Supports embeddings for semantic search
# UNCOMMENT TO USE AZURE OPENAI:
# ConnectionStrings__chat-model=Endpoint=https://YOUR_RESOURCE.openai.azure.com/;AccessKey=YOUR_AZURE_KEY;Model=gpt-4-deployment;Provider=azureopenai

# AZURE AI INFERENCE CONFIGURATION:
#   - Alternative Azure model provider
#   - Similar to Azure OpenAI but different endpoint structure
# UNCOMMENT TO USE AZURE AI INFERENCE:
# ConnectionStrings__chat-model=Endpoint=https://YOUR_RESOURCE.inference.ai.azure.com/;AccessKey=YOUR_KEY;Model=gpt-4o;Provider=azureinference

# LLM response creativity (0.0 = deterministic, 1.0 = balanced, 2.0 = creative)
# Lower values for consistent automation responses, higher for creative writing
LLM_TEMPERATURE=0.7

# Maximum tokens per LLM response
# Lower = faster/cheaper, Higher = more detailed
LLM_MAX_TOKENS=2000


################################################################################
# AGENT CONFIGURATION
################################################################################

# Agent registry service URL
# Docker network: http://lucia-agenthost:5001
# Local: http://localhost:5001
AGENT_REGISTRY_URL=http://localhost:5001

# Timeout for agent-to-agent communication (seconds)
AGENT_TIMEOUT=30

# Retry policy for failed agent calls (none, exponential-backoff, fixed-interval)
AGENT_RETRY_POLICY=exponential-backoff

# Number of retries for failed agent operations
AGENT_MAX_RETRIES=3


################################################################################
# SECURITY & HTTPS
################################################################################

# Enable HTTPS/TLS (false for development, true for production)
ENABLE_HTTPS=false

# TLS certificate path (required if ENABLE_HTTPS=true)
# Docker: /etc/lucia/certs/cert.pem
# Local: ./certs/cert.pem
CERTIFICATE_PATH=/etc/lucia/certs/cert.pem

# TLS private key path (required if ENABLE_HTTPS=true)
# Docker: /etc/lucia/certs/key.pem
# Local: ./certs/key.pem
CERTIFICATE_KEY_PATH=/etc/lucia/certs/key.pem

# CORS allowed origins (comma-separated URLs)
# Development: http://localhost:3000
# Production: https://dashboard.example.com
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000


################################################################################
# OBSERVABILITY & MONITORING
################################################################################

# OpenTelemetry endpoint for distributed tracing
# Local Jaeger: http://localhost:16686
# Datadog: https://api.datadoghq.com
# OTEL Collector: http://otel-collector:4317
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Enable structured logging
LUCIA_STRUCTURED_LOGGING=true

# Application version for telemetry tagging
LUCIA_VERSION=1.0.0


################################################################################
# FEATURE FLAGS
################################################################################

# Enable semantic search for device discovery
FEATURE_SEMANTIC_SEARCH=true

# Enable multi-agent orchestration
FEATURE_MULTI_AGENT_ORCHESTRATION=true

# Enable conversation memory
FEATURE_CONVERSATION_MEMORY=true

# Enable intent caching
FEATURE_INTENT_CACHING=true


################################################################################
# ADVANCED CONFIGURATION
################################################################################

# Chat model concurrency limit (max parallel requests)
CHAT_MODEL_MAX_CONCURRENT_REQUESTS=5

# Request timeout for external API calls (seconds)
REQUEST_TIMEOUT=30

# Buffer size for in-memory conversation history (KB)
CONVERSATION_BUFFER_SIZE=10240

# Enable debug mode (verbose logging, additional error details)
DEBUG_MODE=false

